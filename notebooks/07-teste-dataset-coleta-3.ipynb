{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste com outro Dataset (Coleta 3)\n",
    "\n",
    "## Objetivo\n",
    "Recuperar o modelo treinado no notebook **05-treinamento_classificacao.ipynb** e aplicar o modelo já treinado a um novo conjunto de dados coletados num outro dia para verificar a acurácia.\n",
    "\n",
    "O dataset considerado neste teste usa outras coletas realizadas entre **10:30 do dia 16/11/2022 e 10:15 do dia 17/11/2022**, correspondente a um período de 24 horas. As regras de coletas são as mesmas, coleta a cada 15 minutos durante 24horas.\n",
    "\n",
    "### Métricas Consideradas:\n",
    "- hash (identificador do contêiner)\n",
    "- Consumo de CPU (Valores de pico e tendência central)\n",
    "- Consumo de Memória (Valores de pico e tendência central)\n",
    "- Flavor (rótulo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "import glob\n",
    "import os\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções Gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai a identificação do sistema a partir do nome do ambiente\n",
    "def extract_system_name(namespace):\n",
    "    environment = namespace.split('-')[-1]\n",
    "    system = namespace.split(f'-{environment}')[0]\n",
    "    return system\n",
    "\n",
    "# Cria os diretórios\n",
    "def create_folder(path):\n",
    "    try:\n",
    "       os.makedirs(path)\n",
    "    except FileExistsError:\n",
    "       # directory already exists\n",
    "       pass\n",
    "\n",
    "def hash(ambiente, pod):\n",
    "    return hashlib.md5(f'{ambiente}{pod}'.encode('utf-8')).hexdigest()\n",
    "\n",
    "# Regra de Rotulação\n",
    "def rotular_V2(cpu_MAX, cpu_MED, cpu_threshod, memoria_MAX, memoria_MED, memoria_threshod):\n",
    "    for f in range(0,11):\n",
    "        resultado_memoria = validar_flavor_para_memoria(memoria_MAX, memoria_MED, memoria_threshod, flavors[f])\n",
    "        resultado_cpu = validar_flavor_para_cpu(cpu_MAX, cpu_MED, cpu_threshod, flavors[f])\n",
    "        if resultado_memoria and resultado_cpu:\n",
    "            return flavors[f]['flavor']\n",
    "    return 'Nao classificado'\n",
    "\n",
    "def validar_flavor_para_memoria(memoria_MAX, memoria_MED, memoria_threshod, flavor):\n",
    "    return memoria_MAX + (memoria_threshod * 2 * (memoria_MAX - memoria_MED)) <= flavor['memoria']\n",
    "\n",
    "def validar_flavor_para_cpu(cpu_MAX, cpu_MED,cpu_threshod, flavor):\n",
    "    if cpu_MAX <= flavor['cpu']:\n",
    "        return True\n",
    "    return cpu_MAX - flavor['cpu'] <= cpu_threshod * 2 * (cpu_MAX - cpu_MED)\n",
    "\n",
    "# Predict\n",
    "def predict(model, exemplar):\n",
    "    del exemplar['hash']\n",
    "    del exemplar['flavor']    \n",
    "    print(f'Predicao para os parâmetros [{exemplar}] => flavor {model.predict(exemplar)[0]}')\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def cal_accuracy(y_test, y_pred):      \n",
    "    print(\"Confusion Matrix: \",\n",
    "        confusion_matrix(y_test, y_pred))\n",
    "      \n",
    "    print (\"Accuracy : \",\n",
    "    accuracy_score(y_test,y_pred)*100)\n",
    "      \n",
    "    print(\"Report : \",\n",
    "    classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos arquivos coletados (cpu, memoria, error, throttled)\n",
    "Pré-processamento dos arquivos originais coletados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fim processamento de CPU\n"
     ]
    }
   ],
   "source": [
    "# Processamento de arquivos de CPU\n",
    "header_list = [\"Sistema\", \"Ambiente\", \"Modulo\", \"Pod\", \"Uso_CPU\"]\n",
    "path_processados = '/dados/coletas_dia_3/cpu/processados'\n",
    "path_projeto = '/home/56740050368/Treinamento/IA-PUC_Minas/Trabalho_Cientifico/trabalho_puc_minas_2022'\n",
    "path_metricas = '/dados/coletas_dia_3/cpu/'\n",
    "csv_files = glob.glob(path_projeto+path_metricas+ \"*.gz\")\n",
    "\n",
    "# Cria diretorios\n",
    "create_folder(path_projeto+path_metricas)\n",
    "create_folder(path_projeto+path_processados)\n",
    "\n",
    "for file in csv_files:    \n",
    "    data = pd.read_csv(file, sep=';', header=None, names=header_list)\n",
    "    # obtem horario\n",
    "    filename = file.replace(path_projeto+path_metricas, \"\")\n",
    "    hora = int(filename[4:6])\n",
    "    min = int(filename[6:8])\n",
    "        \n",
    "    # acrescenta coluna de horário    \n",
    "    data['Hora'] = hora\n",
    "    \n",
    "    # acrescenta coluna de minuto    \n",
    "    data['Minuto'] = min\n",
    "    \n",
    "    # acrescenta coluna de hash\n",
    "    data['Hash'] = [hash(x, y) for x, y in zip(data['Ambiente'], data['Pod'])]\n",
    "    \n",
    "    # salva arquivo\n",
    "    data.to_csv(f'{path_projeto}{path_processados}/cpu_{hora}_{min}.csv', index=False)\n",
    "    \n",
    "print('Fim processamento de CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fim processsamento de Memoria\n"
     ]
    }
   ],
   "source": [
    "# Processamento de dados de memória\n",
    "header_list = [\"Sistema\", \"Ambiente\", \"Modulo\", \"Pod\", \"Uso_Memoria\"]\n",
    "path_processados = '/dados/coletas_dia_3/memoria/processados'\n",
    "path_metricas = '/dados/coletas_dia_3/memoria/'\n",
    "csv_files = glob.glob(path_projeto+path_metricas+ \"*.gz\")\n",
    "\n",
    "# Cria diretorios\n",
    "create_folder(path_projeto+path_metricas)\n",
    "create_folder(path_projeto+path_processados)\n",
    "\n",
    "for file in csv_files:    \n",
    "    data = pd.read_csv(file, sep=';', header=None, names=header_list)\n",
    "    # obtem horario\n",
    "    filename = file.replace(path_projeto+path_metricas, \"\")\n",
    "    hora = int(filename[7:9])\n",
    "    min = int(filename[9:11])\n",
    "        \n",
    "    # acrescenta coluna de horário    \n",
    "    data['Hora'] = hora\n",
    "    data['Minuto'] = min\n",
    "    # acrescenta coluna de hash\n",
    "    data['Hash'] = [hash(x, y) for x, y in zip(data['Ambiente'], data['Pod'])]\n",
    "    \n",
    "    # salva arquivo\n",
    "    data.to_csv(f'{path_projeto}{path_processados}/memoria_{hora}_{min}.csv', index=False)\n",
    "    \n",
    "print('Fim processsamento de Memoria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fim processsamento de Erros de Memória\n"
     ]
    }
   ],
   "source": [
    "# Processamento da coleta de pods encerrados por estouro de memória\n",
    "header_list = [\"Sistema\", \"Ambiente\", \"Modulo\", \"Pod\", \"Error\", \"Qtd\"]\n",
    "path_processados = '/dados/coletas_dia_3/error/processados'\n",
    "path_metricas = '/dados/coletas_dia_3/error/'\n",
    "csv_files = glob.glob(path_projeto+path_metricas+ \"*.gz\")\n",
    "\n",
    "# Cria diretorios\n",
    "create_folder(path_projeto+path_metricas)\n",
    "create_folder(path_projeto+path_processados)\n",
    "\n",
    "for file in csv_files:    \n",
    "    data = pd.read_csv(file, sep=';', header=None, names=header_list)\n",
    "    # obtem horario\n",
    "    filename = file.replace(path_projeto+path_metricas, \"\")\n",
    "    \n",
    "    hora = int(filename[6:8])\n",
    "    min = int(filename[8:10])   \n",
    "        \n",
    "    # acrescenta coluna de horário    \n",
    "    data['Hora'] = hora\n",
    "    data['Minuto'] = min\n",
    "    # acrescenta coluna de hash\n",
    "    data['Hash'] = [hash(x, y) for x, y in zip(data['Ambiente'], data['Pod'])]\n",
    "    \n",
    "    # salva arquivo\n",
    "    data.to_csv(f'{path_projeto}{path_processados}/error_{hora}_{min}.csv', index=False)\n",
    "    \n",
    "print('Fim processsamento de Erros de Memória')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fim processsamento de CPU Throttled\n"
     ]
    }
   ],
   "source": [
    "# Processamento da coleta de pods encerrados por estouro de memória\n",
    "header_list = [\"Sistema\", \"Ambiente\", \"Modulo\", \"Pod\", \"Uso_CPU\"]\n",
    "path_processados = '/dados/coletas_dia_3/throttled/processados'\n",
    "path_metricas = '/dados/coletas_dia_3/throttled/'\n",
    "csv_files = glob.glob(path_projeto+path_metricas+ \"*.gz\")\n",
    "\n",
    "# Cria diretorios\n",
    "create_folder(path_projeto+path_metricas)\n",
    "create_folder(path_projeto+path_processados)\n",
    "\n",
    "for file in csv_files:    \n",
    "    data = pd.read_csv(file, sep=';', header=None, names=header_list)\n",
    "    # obtem horario\n",
    "    filename = file.replace(path_projeto+path_metricas, \"\")\n",
    "    hora = int(filename[14:16])\n",
    "    min = int(filename[16:18])   \n",
    "        \n",
    "    # acrescenta coluna de horário    \n",
    "    data['Hora'] = hora\n",
    "    data['Minuto'] = min\n",
    "    # acrescenta coluna de hash\n",
    "    data['Hash'] = [hash(x, y) for x, y in zip(data['Ambiente'], data['Pod'])]\n",
    "    \n",
    "    # salva arquivo\n",
    "    data.to_csv(f'{path_projeto}{path_processados}/cpu_throttled_{hora}_{min}.csv', index=False)\n",
    "    \n",
    "print('Fim processsamento de CPU Throttled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidação das Métricas num único arquivo por métrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerado arquivo consolidado da métrica de consumo de CPU\n"
     ]
    }
   ],
   "source": [
    "# Recuperar metricas dos arquivos processados\n",
    "path_processados = '/dados/coletas_dia_3/cpu/processados'\n",
    "csv_files = glob.glob(path_projeto+path_processados + \"/*.csv\")\n",
    "\n",
    "df_list = (pd.read_csv(file, skiprows = 1,header = None) for file in csv_files)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "big_cpu_df  = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Cria Tb Aplicações\n",
    "tb_medidas_cpu = big_cpu_df[[7, 5, 6, 4]].copy() # Colunas hash, hora, minuto, consumo_cpu\n",
    "\n",
    "# salva arquivo\n",
    "tb_medidas_cpu.to_csv(f'{path_projeto}/{path_processados}/consolidado_cpu.csv', index=False, header=['hash', 'hora', 'min', 'consumo_cpu']) # \n",
    "    \n",
    "print('Gerado arquivo consolidado da métrica de consumo de CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerado arquivo consolidado da métrica de consumo de Memória\n"
     ]
    }
   ],
   "source": [
    "# Recuperar metricas dos arquivos processados\n",
    "path_processados = '/dados/coletas_dia_3/memoria/processados'\n",
    "csv_files = glob.glob(path_projeto+path_processados + \"/*.csv\")\n",
    "\n",
    "df_list = (pd.read_csv(file, skiprows = 1,header = None) for file in csv_files)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "big_memoria_df  = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Cria Tb Aplicações\n",
    "tb_medidas_memoria = big_memoria_df[[7, 5, 6, 4]].copy() # Colunas hash, hora, minuto, consumo_memoria\n",
    "\n",
    "# salva arquivo\n",
    "tb_medidas_memoria.to_csv(f'{path_projeto}/{path_processados}/consolidado_memoria.csv', index=False, header=['hash', 'hora', 'min', 'consumo_memoria']) \n",
    "    \n",
    "print('Gerado arquivo consolidado da métrica de consumo de Memória')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerado arquivo consolidado da métrica de erros de Memória\n"
     ]
    }
   ],
   "source": [
    "# Recuperar metricas dos arquivos processados\n",
    "path_processados = '/dados/coletas_dia_3/error/processados'\n",
    "csv_files = glob.glob(path_projeto+path_processados + \"/*.csv\")\n",
    "\n",
    "df_list = (pd.read_csv(file, skiprows = 1,header = None) for file in csv_files)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "big_error_df  = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Cria Tb Aplicações\n",
    "tb_medidas_error = big_error_df[[8, 6, 7, 5]].copy() # Colunas hash, hora, minuto, qtd de erros\n",
    "\n",
    "# salva arquivo\n",
    "tb_medidas_error.to_csv(f'{path_projeto}/{path_processados}/consolidado_error.csv', index=False, header=['hash', 'hora', 'min', 'qtd']) \n",
    "    \n",
    "print('Gerado arquivo consolidado da métrica de erros de Memória')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerado arquivo consolidado da métrica de consumo excessivo de cpu\n"
     ]
    }
   ],
   "source": [
    "# Recuperar metricas dos arquivos processados\n",
    "path_processados = '/dados/coletas_dia_3/throttled/processados'\n",
    "csv_files = glob.glob(path_projeto+path_processados + \"/*.csv\")\n",
    "\n",
    "df_list = (pd.read_csv(file, skiprows = 1,header = None) for file in csv_files)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "big_throttled_df  = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Cria Tb Aplicações\n",
    "tb_medidas_throttled = big_throttled_df[[7, 5, 6, 4]].copy() # Colunas hash, hora, minuto, qtd de erros\n",
    "\n",
    "# salva arquivo\n",
    "tb_medidas_throttled.to_csv(f'{path_projeto}/{path_processados}/consolidado_cpu_throttled.csv', index=False, header=['hash', 'hora', 'min', 'consumo_cpu']) \n",
    "    \n",
    "print('Gerado arquivo consolidado da métrica de consumo excessivo de cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregação dos Dados e Limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerados valores de CPU: valor de pico e mediana\n"
     ]
    }
   ],
   "source": [
    "## Agregação dos valores de CPU\n",
    "path_processados_cpu = '/dados/coletas_dia_3/cpu/processados'\n",
    "csv_file_consolidado_cpu = f'{path_projeto}{path_processados_cpu}/consolidado_cpu.csv'\n",
    "df_consolidado_cpu = pd.read_csv(csv_file_consolidado_cpu, sep=',',decimal='.')\n",
    "# Remoção dos valores zerados de consumo de CPU\n",
    "df_consolidado_cpu = df_consolidado_cpu[df_consolidado_cpu['consumo_cpu'] > 0]\n",
    "# Transformação do consumo de CPU em miliCPU (miliCPU) (x 1000)\n",
    "df_consolidado_cpu['consumo_cpu'] = df_consolidado_cpu['consumo_cpu'] * 1000\n",
    "# Exclui as colunas de hora e minuto\n",
    "del df_consolidado_cpu['hora']\n",
    "del df_consolidado_cpu['min']\n",
    "# Realiza agrupamento do consumo de CPU por aplicação, identififcado pela coluna hash. \n",
    "# Será considerado os valores de pico (max) por aplicação\n",
    "df_group_by_max_cpu = df_consolidado_cpu.groupby(['hash']).max()\n",
    "# Reseta o índice\n",
    "df_group_by_max_cpu.reset_index(inplace=True)\n",
    "df_group_by_max_cpu = df_group_by_max_cpu.rename(columns = {'consumo_cpu':'pico_cpu'})\n",
    "# Agrupa as aplicações e agrega pelo valor da mediana dos valores de consumo de CPU por aplicação\n",
    "df_group_by_med_cpu = df_consolidado_cpu.groupby(['hash']).median()\n",
    "# Reseta o índice\n",
    "df_group_by_med_cpu.reset_index(inplace=True)\n",
    "df_group_by_med_cpu = df_group_by_med_cpu.rename(columns = {'consumo_cpu':'mediana_cpu'})\n",
    "print('Gerados valores de CPU: valor de pico e mediana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerados valores de Memoria: valor de pico e mediana\n"
     ]
    }
   ],
   "source": [
    "## Agregação dos dados de Memória\n",
    "path_processados_memoria = '/dados/coletas_dia_3/memoria/processados'\n",
    "csv_file_consolidado_memoria = f'{path_projeto}{path_processados_memoria}/consolidado_memoria.csv'\n",
    "df_consolidado_memoria = pd.read_csv(csv_file_consolidado_memoria, sep=',',decimal='.')\n",
    "# Remove os valores zerados de consumo de memória\n",
    "df_consolidado_memoria = df_consolidado_memoria[df_consolidado_memoria['consumo_memoria'] > 0]\n",
    "# Exclui a coluna de hora e minuto\n",
    "del df_consolidado_memoria['hora']\n",
    "del df_consolidado_memoria['min']\n",
    "## Connverte o valor de consumo de memória de bytes para MB\n",
    "df_consolidado_memoria['consumo_memoria'] = df_consolidado_memoria['consumo_memoria'] / (1024 * 1024)\n",
    "# Obtenção do Pico de Memória por aplicação\n",
    "df_group_by_max_memoria = df_consolidado_memoria.groupby(['hash']).max()\n",
    "# Reseta o índice\n",
    "df_group_by_max_memoria.reset_index(inplace=True)\n",
    "df_group_by_max_memoria = df_group_by_max_memoria.rename(columns = {'consumo_memoria':'pico_memoria'})\n",
    "# Obtenção da Mediana de Consumo de Memória\n",
    "df_group_by_med_memoria = df_consolidado_memoria.groupby(['hash']).median()\n",
    "# Reseta o índice\n",
    "df_group_by_med_memoria.reset_index(inplace=True)\n",
    "df_group_by_med_memoria = df_group_by_med_memoria.rename(columns = {'consumo_memoria':'mediana_memoria'})\n",
    "print('Gerados valores de Memoria: valor de pico e mediana')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregação das métricas num único dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>mediana_cpu</th>\n",
       "      <th>pico_cpu</th>\n",
       "      <th>mediana_memoria</th>\n",
       "      <th>pico_memoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00115d136a63167e961d60147809aff7</td>\n",
       "      <td>129.762665</td>\n",
       "      <td>255.885808</td>\n",
       "      <td>208.097656</td>\n",
       "      <td>208.460938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00171f1ecb4cb384c73b460e424033ae</td>\n",
       "      <td>10.674630</td>\n",
       "      <td>17.904220</td>\n",
       "      <td>129.964844</td>\n",
       "      <td>138.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00190e6b7ce5fa82d9e75b96948f244e</td>\n",
       "      <td>3.239884</td>\n",
       "      <td>3.807161</td>\n",
       "      <td>127.517578</td>\n",
       "      <td>130.609375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00192f6f6f4e5a93d8d562202c6d0227</td>\n",
       "      <td>0.856028</td>\n",
       "      <td>0.939062</td>\n",
       "      <td>485.562500</td>\n",
       "      <td>485.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001d79ecf18bc2bb98c8ac7dc5ab4526</td>\n",
       "      <td>2.546197</td>\n",
       "      <td>4.612576</td>\n",
       "      <td>127.632812</td>\n",
       "      <td>131.617188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13352</th>\n",
       "      <td>ffd950eb6ceeacfbd2f239eb34679cbe</td>\n",
       "      <td>1.043283</td>\n",
       "      <td>3.513072</td>\n",
       "      <td>674.507812</td>\n",
       "      <td>674.777344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13353</th>\n",
       "      <td>ffdb935e97a6dd0530958526e31dc129</td>\n",
       "      <td>3.342989</td>\n",
       "      <td>7.209065</td>\n",
       "      <td>732.753906</td>\n",
       "      <td>1022.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13354</th>\n",
       "      <td>ffe7450b3571c80c26389cd403b6017c</td>\n",
       "      <td>3.338460</td>\n",
       "      <td>7.572723</td>\n",
       "      <td>180.544922</td>\n",
       "      <td>249.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13355</th>\n",
       "      <td>fff13383e3a6e8b8cc87a597f9fc8272</td>\n",
       "      <td>4.123843</td>\n",
       "      <td>61.843285</td>\n",
       "      <td>3197.683594</td>\n",
       "      <td>3253.917969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13356</th>\n",
       "      <td>fff602eaae3069b08241eb3d38862349</td>\n",
       "      <td>178.627170</td>\n",
       "      <td>227.864866</td>\n",
       "      <td>3047.554688</td>\n",
       "      <td>4089.542969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13357 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   hash  mediana_cpu    pico_cpu  \\\n",
       "0      00115d136a63167e961d60147809aff7   129.762665  255.885808   \n",
       "1      00171f1ecb4cb384c73b460e424033ae    10.674630   17.904220   \n",
       "2      00190e6b7ce5fa82d9e75b96948f244e     3.239884    3.807161   \n",
       "3      00192f6f6f4e5a93d8d562202c6d0227     0.856028    0.939062   \n",
       "4      001d79ecf18bc2bb98c8ac7dc5ab4526     2.546197    4.612576   \n",
       "...                                 ...          ...         ...   \n",
       "13352  ffd950eb6ceeacfbd2f239eb34679cbe     1.043283    3.513072   \n",
       "13353  ffdb935e97a6dd0530958526e31dc129     3.342989    7.209065   \n",
       "13354  ffe7450b3571c80c26389cd403b6017c     3.338460    7.572723   \n",
       "13355  fff13383e3a6e8b8cc87a597f9fc8272     4.123843   61.843285   \n",
       "13356  fff602eaae3069b08241eb3d38862349   178.627170  227.864866   \n",
       "\n",
       "       mediana_memoria  pico_memoria  \n",
       "0           208.097656    208.460938  \n",
       "1           129.964844    138.015625  \n",
       "2           127.517578    130.609375  \n",
       "3           485.562500    485.734375  \n",
       "4           127.632812    131.617188  \n",
       "...                ...           ...  \n",
       "13352       674.507812    674.777344  \n",
       "13353       732.753906   1022.218750  \n",
       "13354       180.544922    249.750000  \n",
       "13355      3197.683594   3253.917969  \n",
       "13356      3047.554688   4089.542969  \n",
       "\n",
       "[13357 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizando o join do consumo de memória e cpu\n",
    "# Agregação das colunas de valores mediana e pico da CPU\n",
    "df_group_by_cpu = pd.concat([df_group_by_med_cpu.set_index('hash'),df_group_by_max_cpu.set_index('hash')], \n",
    "                            axis=1, join='inner').reset_index()\n",
    "\n",
    "# Agregação das colunas de valores mediana e pico da Memoria\n",
    "df_group_by_memoria = pd.concat([df_group_by_med_memoria.set_index('hash'),df_group_by_max_memoria.set_index('hash')], \n",
    "                                axis=1, join='inner').reset_index()\n",
    "\n",
    "# Agregação das colunas de CPU e Memoria\n",
    "df_group_by_cpu_memoria = pd.concat([df_group_by_cpu.set_index('hash'),df_group_by_memoria.set_index('hash')], \n",
    "                                    axis=1, join='inner').reset_index()\n",
    "df_group_by_cpu_memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção da aplicações com execuções além dos recursos de cpu e memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removido aplicacoes (conteineres) com execuções além dos recursos de cpu e memoria\n"
     ]
    }
   ],
   "source": [
    "## Agrupa as aplicacoes com estouro de memória\n",
    "path_processados_error = '/dados/coletas_dia_3/error/processados'\n",
    "csv_file_consolidado_error = f'{path_projeto}{path_processados_error}/consolidado_error.csv'\n",
    "df_consolidado_error_memoria = pd.read_csv(csv_file_consolidado_error, sep=',',decimal='.')\n",
    "## Agrega as aplicações pelo o hash de identificação\n",
    "df_aplicacoes_error_memoria = df_consolidado_error_memoria['hash'].unique()\n",
    "## Remove as aplicaçoes com erro de memória do dataframe consolidado\n",
    "df_group_by_cpu_memoria = df_group_by_cpu_memoria[~df_group_by_cpu_memoria['hash'].isin(df_aplicacoes_error_memoria)]\n",
    "## Agrupa as aplicacoes com excesso de cpu (throttled)\n",
    "path_processados_throttled = '/dados/coletas_dia_3/throttled/processados'\n",
    "csv_file_consolidado_throttled = f'{path_projeto}{path_processados_throttled}/consolidado_cpu_throttled.csv'\n",
    "df_consolidado_throttled = pd.read_csv(csv_file_consolidado_throttled, sep=',',decimal='.')\n",
    "## Agrega as aplicações pelo o hash de identificação\n",
    "df_consolidado_throttled = df_consolidado_throttled['hash'].unique()\n",
    "## Remove as aplicaçoes com erro de memória do dataframe consolidado\n",
    "df_group_by_cpu_memoria = df_group_by_cpu_memoria[~df_group_by_cpu_memoria['hash'].isin(df_consolidado_throttled)]\n",
    "df_group_by_cpu_memoria.reset_index(drop=True)\n",
    "print('Removido aplicacoes (conteineres) com execuções além dos recursos de cpu e memoria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remoção de valores discrepantes (acima do flavor f10)\n",
    "df_group_by_cpu_memoria = df_group_by_cpu_memoria[df_group_by_cpu_memoria['pico_cpu'] < 10.8]\n",
    "df_group_by_cpu_memoria = df_group_by_cpu_memoria[df_group_by_cpu_memoria['pico_memoria'] < 550]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotulação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotulacao Completa!\n"
     ]
    }
   ],
   "source": [
    "## Flavors\n",
    "flavors = [{'id': '0', 'flavor': 'f0', 'cpu': 1, 'memoria': 50},\n",
    "           {'id': '1', 'flavor': 'f1', 'cpu': 2, 'memoria': 100},\n",
    "           {'id': '2', 'flavor': 'f2', 'cpu': 3, 'memoria': 150},\n",
    "           {'id': '3', 'flavor': 'f3', 'cpu': 4, 'memoria': 200},\n",
    "           {'id': '4', 'flavor': 'f4', 'cpu': 5, 'memoria': 250},\n",
    "           {'id': '5', 'flavor': 'f5', 'cpu': 6, 'memoria': 300},\n",
    "           {'id': '6', 'flavor': 'f6', 'cpu': 7, 'memoria': 350},\n",
    "           {'id': '7', 'flavor': 'f7', 'cpu': 8, 'memoria': 400},\n",
    "           {'id': '8', 'flavor': 'f8', 'cpu': 9, 'memoria': 450},\n",
    "           {'id': '9', 'flavor': 'f9', 'cpu': 10, 'memoria': 500},\n",
    "           {'id': '10', 'flavor': 'f10', 'cpu': 11, 'memoria': 550}]\n",
    "\n",
    "## Thresholds\n",
    "cpu_threshod = 0.05\n",
    "memoria_threshod = 0.1\n",
    "\n",
    "## Atribuição do flavor\n",
    "df_group_by_cpu_memoria['flavor'] = df_group_by_cpu_memoria.apply(lambda x: rotular_V2(x.pico_cpu, x.mediana_cpu, cpu_threshod, x.pico_memoria, x.mediana_memoria, memoria_threshod), axis=1)\n",
    "print ('Rotulacao Completa!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de conteineres não classificados  2\n"
     ]
    }
   ],
   "source": [
    "print('Quantidade de conteineres não classificados ', len(df_group_by_cpu_memoria[df_group_by_cpu_memoria['flavor']=='Nao classificado']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>mediana_cpu</th>\n",
       "      <th>pico_cpu</th>\n",
       "      <th>mediana_memoria</th>\n",
       "      <th>pico_memoria</th>\n",
       "      <th>flavor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>546937e47847d74a02674f17a74536e8</td>\n",
       "      <td>1.431043</td>\n",
       "      <td>2.274491</td>\n",
       "      <td>549.464844</td>\n",
       "      <td>549.925781</td>\n",
       "      <td>Nao classificado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>7bf6caf2f2abdc66445c0a231cd580dd</td>\n",
       "      <td>3.974121</td>\n",
       "      <td>5.606177</td>\n",
       "      <td>548.808594</td>\n",
       "      <td>549.972656</td>\n",
       "      <td>Nao classificado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  hash  mediana_cpu  pico_cpu  \\\n",
       "4481  546937e47847d74a02674f17a74536e8     1.431043  2.274491   \n",
       "6567  7bf6caf2f2abdc66445c0a231cd580dd     3.974121  5.606177   \n",
       "\n",
       "      mediana_memoria  pico_memoria            flavor  \n",
       "4481       549.464844    549.925781  Nao classificado  \n",
       "6567       548.808594    549.972656  Nao classificado  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Remove os não classificados\n",
    "## Os não classificados deveriam estar num próximo flavor f11, caso existisse. Neste caso serão removidos.\n",
    "df_nao_classificado = df_group_by_cpu_memoria[df_group_by_cpu_memoria['flavor']=='Nao classificado']\n",
    "df_nao_classificado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificação dos Dados com o Modelo já treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregado o modelo :  DecisionTreeClassifier(criterion='entropy')\n"
     ]
    }
   ],
   "source": [
    "## Carrega o modelo\n",
    "# Loading the saved decision tree model pickle\n",
    "import pickle\n",
    "decision_tree_pkl_filename = 'decision_tree_classifier_entropy.pkl'\n",
    "decision_tree_model_pkl = open(decision_tree_pkl_filename, 'rb')\n",
    "decision_tree_model = pickle.load(decision_tree_model_pkl)\n",
    "print (\"Carregado o modelo : \", decision_tree_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predições e Validação com o novo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>mediana_cpu</th>\n",
       "      <th>pico_cpu</th>\n",
       "      <th>mediana_memoria</th>\n",
       "      <th>pico_memoria</th>\n",
       "      <th>flavor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00190e6b7ce5fa82d9e75b96948f244e</td>\n",
       "      <td>3.239884</td>\n",
       "      <td>3.807161</td>\n",
       "      <td>127.517578</td>\n",
       "      <td>130.609375</td>\n",
       "      <td>f3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00192f6f6f4e5a93d8d562202c6d0227</td>\n",
       "      <td>0.856028</td>\n",
       "      <td>0.939062</td>\n",
       "      <td>485.562500</td>\n",
       "      <td>485.734375</td>\n",
       "      <td>f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001d79ecf18bc2bb98c8ac7dc5ab4526</td>\n",
       "      <td>2.546197</td>\n",
       "      <td>4.612576</td>\n",
       "      <td>127.632812</td>\n",
       "      <td>131.617188</td>\n",
       "      <td>f4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00397e755d86f88d58f23419ce8ba8e8</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.002905</td>\n",
       "      <td>11.078125</td>\n",
       "      <td>11.082031</td>\n",
       "      <td>f0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>004156954736d58869a3a2e1d5e197e8</td>\n",
       "      <td>6.623067</td>\n",
       "      <td>7.728354</td>\n",
       "      <td>353.800781</td>\n",
       "      <td>354.683594</td>\n",
       "      <td>f7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                hash  mediana_cpu  pico_cpu  mediana_memoria  \\\n",
       "2   00190e6b7ce5fa82d9e75b96948f244e     3.239884  3.807161       127.517578   \n",
       "3   00192f6f6f4e5a93d8d562202c6d0227     0.856028  0.939062       485.562500   \n",
       "4   001d79ecf18bc2bb98c8ac7dc5ab4526     2.546197  4.612576       127.632812   \n",
       "9   00397e755d86f88d58f23419ce8ba8e8     0.001729  0.002905        11.078125   \n",
       "10  004156954736d58869a3a2e1d5e197e8     6.623067  7.728354       353.800781   \n",
       "\n",
       "    pico_memoria flavor  \n",
       "2     130.609375     f3  \n",
       "3     485.734375     f9  \n",
       "4     131.617188     f4  \n",
       "9      11.082031     f0  \n",
       "10    354.683594     f7  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group_by_cpu_memoria.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicao para os parâmetros [   mediana_cpu  pico_cpu  mediana_memoria  pico_memoria\n",
      "3     0.856028  0.939062         485.5625    485.734375] => flavor f9\n"
     ]
    }
   ],
   "source": [
    "# Teste com escolha manual\n",
    "exemplar = df_group_by_cpu_memoria[df_group_by_cpu_memoria['hash']=='00192f6f6f4e5a93d8d562202c6d0227']\n",
    "predict(decision_tree_model, exemplar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicao para os parâmetros [   mediana_cpu  pico_cpu  mediana_memoria  pico_memoria\n",
      "9     0.001729  0.002905        11.078125     11.082031] => flavor f0\n"
     ]
    }
   ],
   "source": [
    "# Teste com escolha manual\n",
    "exemplar = df_group_by_cpu_memoria[df_group_by_cpu_memoria['hash']=='00397e755d86f88d58f23419ce8ba8e8']\n",
    "predict(decision_tree_model, exemplar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de conteineres não classificados  0\n"
     ]
    }
   ],
   "source": [
    "## Remove não classificados\n",
    "for i in df_nao_classificado.index:\n",
    "    df_group_by_cpu_memoria = df_group_by_cpu_memoria.drop([i])\n",
    "print('Quantidade de conteineres não classificados ', len(df_group_by_cpu_memoria[df_group_by_cpu_memoria['flavor']=='Nao classificado']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9792554737995662"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = df_group_by_cpu_memoria[['mediana_cpu', 'pico_cpu', 'mediana_memoria', 'pico_memoria']]\n",
    "y_test = df_group_by_cpu_memoria[['flavor']]\n",
    "allScores = cross_val_score(decision_tree_model, X_test, y_test , cv=10)\n",
    "# cross_val_score retorna array com as 10 validações\n",
    "allScores.mean() # tomamos a média do score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:  [[1703    0    0    0    0    0    0    0    0    0    0]\n",
      " [   2 1172    0    6    0    0    0    0    0    0    0]\n",
      " [   0    0  277    0    0    0    0    0    0    0   12]\n",
      " [   0    6    0  763    6    0    0    0    0    0    0]\n",
      " [   0    0    0    8  531   10    0    0    0    0    0]\n",
      " [   0    0    0    0    5  489    5    0    0    0    0]\n",
      " [   0    0    0    0    0    5  449   10    0    0    0]\n",
      " [   0    0    0    0    0    0    5  452    9    0    0]\n",
      " [   0    0    0    0    0    0    0    7  517   13    0]\n",
      " [   0    0    0    0    0    0    0    0   10  441   12]\n",
      " [   0    0   18    0    0    0    0    0    1   16  366]]\n",
      "Accuracy :  97.73409773409773\n",
      "Report :                precision    recall  f1-score   support\n",
      "\n",
      "          f0       1.00      1.00      1.00      1703\n",
      "          f1       0.99      0.99      0.99      1180\n",
      "         f10       0.94      0.96      0.95       289\n",
      "          f2       0.98      0.98      0.98       775\n",
      "          f3       0.98      0.97      0.97       549\n",
      "          f4       0.97      0.98      0.98       499\n",
      "          f5       0.98      0.97      0.97       464\n",
      "          f6       0.96      0.97      0.97       466\n",
      "          f7       0.96      0.96      0.96       537\n",
      "          f8       0.94      0.95      0.95       463\n",
      "          f9       0.94      0.91      0.93       401\n",
      "\n",
      "    accuracy                           0.98      7326\n",
      "   macro avg       0.97      0.97      0.97      7326\n",
      "weighted avg       0.98      0.98      0.98      7326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "y_pred = decision_tree_model.predict(X_test)\n",
    "cal_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
